{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders\n",
    "\n",
    "\n",
    "В этом ноутбуке описывается процесс формирования обучающей выборки для обучения модели классификации жестов в дальнейшем.\n",
    "\n",
    "Для запуска данного ноутбука необходимо выполнить следующие команды:\n",
    "\n",
    "1. Загрузить данные с помощью команды ниже — для этого потребуется около 90 GB на диске или виртуальном хранилище.\n",
    "```bash\n",
    "sh download_data.sh\n",
    "```\n",
    "\n",
    "    - Данная команда автоматически загрузит данные в директорию `./INPUT_DATA/TRAIN_DATA/`.\n",
    "\n",
    "    - Скачанные `zip`- архивы будут доступны в директории `./INPUT_DATA/ZIP/`\n",
    "\n",
    "2. Установить зависимости:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Алгоритм\n",
    "\n",
    "## Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2021-07-03 02:19:44\", \"name\": \"matplotlib.pyplot\", \"filename\": \"pyplot.py\", \"levelname\": \"DEBUG\", \"message\": \"Loaded backend module://matplotlib_inline.backend_inline version unknown.\"}\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import face_detection as fd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('.')\n",
    "from definitions import ROOT_DIR\n",
    "from detectime.augmentations import (\n",
    "    result_crop\n",
    ")\n",
    "from detectime.maskrcnn import (\n",
    "    load_model_custom\n",
    ")\n",
    "from mrcnn.config import Config\n",
    "from detectime.utils import read_image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропишем пути к источникам данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_DIR / 'data'\n",
    "NOTEBOOK_PATH = ROOT_DIR / 'notebooks'\n",
    "INPUT_DATA = DATA_PATH / 'INPUT_DATA'\n",
    "INPUT_IMAGES_FOLDER = INPUT_DATA / 'TRAIN_DATA'\n",
    "TRAIN_IMG_FOLDER = INPUT_DATA / 'TRAIN_IMG'\n",
    "SAVE_TRAIN_IMAGES_HANDS = TRAIN_IMG_FOLDER / 'HANDS'\n",
    "SAVE_TRAIN_IMAGES_FACES= TRAIN_IMG_FOLDER / 'FACES'\n",
    "\n",
    "JSON_FOLDER = INPUT_DATA / 'JSON'\n",
    "FACES_JSON_PRETRAINED = JSON_FOLDER / 'train_with_bboxes.json'\n",
    "TRAIN_LABELS = INPUT_DATA / 'train.csv'\n",
    "HAND_DETECTION_FOLDER = ROOT_DIR / 'model' / 'mask_rcnn_hand_detection.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим основной конфиг из файла `./config.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from detectime.utils import convert_dict_to_tuple\n",
    "\n",
    "CONFIG_PATH = ROOT_DIR / 'config.yml'\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    data = yaml.safe_load(f)\n",
    "config = convert_dict_to_tuple(dictionary=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Загрузим все необходимое для модели обнаружения лиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "\n",
    "model_detector_faces = fd.build_detector(\n",
    "        config.detection.detector_type,\n",
    "        confidence_threshold=.5,\n",
    "        nms_iou_threshold=.3,\n",
    "        device=device,\n",
    "        max_resolution=640\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь все необходимое для модуля обнаружения рук.\n",
    "Параметр `DETECTION_MIN_CONFIDENCE` можно варьировать, но все же выставим\n",
    "большую вероятность того, что найденный объект является рукой - так мы повысим\n",
    "качество выборки для обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, 1024)\n",
      "(None, None, None, 1024)\n"
     ]
    }
   ],
   "source": [
    "class HandConfig(Config):\n",
    "    NAME = \"hand\"\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    STEPS_PER_EPOCH = 10\n",
    "    DETECTION_MIN_CONFIDENCE = 0.99\n",
    "\n",
    "\n",
    "class InferenceConfig(HandConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "model_detector_hands = load_model_custom(\n",
    "    InferenceConfig(),\n",
    "    str(HAND_DETECTION_FOLDER)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, способ работы которой описан выше в разделе *Алгоритм*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_data(data_df,\n",
    "                   detector_faces,\n",
    "                   detector_hands,\n",
    "                   output_path,\n",
    "                   crop_coefficient=1.5,\n",
    "                   crop_hand_coefficient=1.5,\n",
    "                   return_data=True):\n",
    "\n",
    "    result_arr = []\n",
    "    print(f'Savedir hands: {output_path}')\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    for idx, image_path, label in tqdm(zip(list(data_df.index),\n",
    "                                    data_df.frame_path.values,\n",
    "                                    data_df.label.values), total=len(data_df)):\n",
    "        img_path = str(INPUT_IMAGES_FOLDER / image_path)\n",
    "        if not os.path.isfile(img_path):\n",
    "            print(f'NO SUCH FILE {img_path}')\n",
    "        else:\n",
    "            img = read_image(img_path)\n",
    "            # DETECTOR FACES\n",
    "            detections = detector_faces.detect(img)\n",
    "            all_faces = []\n",
    "            for det in detections:\n",
    "                x1, y1, x2, y2, s = det.tolist()\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                bbox = [round(x1), round(y1), round(w), round(h)]\n",
    "                all_faces.append(bbox)\n",
    "            # DETECT HANDS\n",
    "            result = detector_hands.detect([img], verbose=0)[0]\n",
    "            all_hands = result['rois']\n",
    "\n",
    "            # AREA\n",
    "            area = []\n",
    "            faces_and_hands = []\n",
    "\n",
    "            # CROP FACES\n",
    "            for face in all_faces:\n",
    "                for hand in all_hands:\n",
    "                    x3, y3, x4, y4 = result_crop(\n",
    "                        img,\n",
    "                        face,\n",
    "                        crop_coefficient=crop_coefficient\n",
    "                    )\n",
    "                    y1, x1, y2, x2 = hand\n",
    "\n",
    "                    left_x, left_y = max(x1, x3), max(y1, y3)\n",
    "                    right_x, right_y = min(x2, x4), min(y2, y4)\n",
    "\n",
    "                    width, height = right_x - left_x, right_y - left_y\n",
    "                    if width <= 0 or height <= 0:\n",
    "                        area.append(0)\n",
    "                    else:\n",
    "                        area.append(width * height)\n",
    "                    faces_and_hands.append((face, hand))\n",
    "            if area:\n",
    "                max_area = max(area)\n",
    "                if max_area != 0:\n",
    "                    index_of_max_area = area.index(max_area)\n",
    "                    index_of_face_hand = faces_and_hands[index_of_max_area]\n",
    "                    final_face, final_hand = index_of_face_hand\n",
    "                    y1, x1, y2, x2 = final_hand\n",
    "                    if label == 3:\n",
    "                        x1, y1, x2, y2 = result_crop(img,\n",
    "                                                     [x1, y1, x2-x1, y2-y1],\n",
    "                                                     crop_coefficient=crop_hand_coefficient\n",
    "                                                     )\n",
    "                    item = {\n",
    "                        'frame_path': image_path,\n",
    "                        'video_name': data_df.video_name.iloc[idx],\n",
    "                        'frame_id': int(data_df.frame_id.iloc[idx]),\n",
    "                        'label': int(label),\n",
    "                        'bbox': [int(x1), int(y1), int(x2), int(y2)]\n",
    "                    }\n",
    "                    result_arr.append(item)\n",
    "\n",
    "    with open(str(output_path / 'hands.json'), 'w') as file:\n",
    "        json.dump(result_arr, file, indent=4)\n",
    "    if return_data:\n",
    "        return result_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAME2LABEL_DICT = {\n",
    "    'no_gesture': 0,\n",
    "    'stop': 1,\n",
    "    'victory': 2,\n",
    "    'mute': 3,\n",
    "    'ok': 4,\n",
    "    'like': 5,\n",
    "    'dislike': 6\n",
    "}\n",
    "\n",
    "train_data = pd.read_csv(str(TRAIN_LABELS))\n",
    "train_data['label'] = train_data['class_name'].map(CLASS_NAME2LABEL_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исключим данные, помеченные в качестве `no_gesture` - будем получать данные только для 6 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175174, 5)\n"
     ]
    }
   ],
   "source": [
    "train_data_gestures = train_data[train_data['label'] != 0]\n",
    "print(train_data_gestures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "215"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "exists_pictures = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_IMAGES_FOLDER):\n",
    "    for filename in filenames:\n",
    "        video_plus_picture = list(Path(os.path.join(dirname, filename)).parts)[-2:]\n",
    "        frame_path = '/'.join(video_plus_picture)\n",
    "        exists_pictures.append(\n",
    "            frame_path\n",
    "        )\n",
    "\n",
    "len(exists_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(163, 5)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = (\n",
    "    train_data_gestures[\n",
    "        train_data_gestures['frame_path']\n",
    "            .isin(exists_pictures)\n",
    "    ].reset_index(drop=True)\n",
    ")\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Начнем получение тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = get_train_data(train,\n",
    "                      model_detector_faces,\n",
    "                      model_detector_hands,\n",
    "                      JSON_FOLDER,\n",
    "                      crop_coefficient=1.5,\n",
    "                      crop_hand_coefficient=1.5,\n",
    "                      return_data=True\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f3bbdf2d",
   "language": "python",
   "display_name": "PyCharm (detectime)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}